{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c912b931",
   "metadata": {},
   "source": [
    "# Run HSIDwRD Codespace on Kaggle\n",
    "This notebook clones your GitHub repo, installs dependencies, and runs your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7504f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clone the GitHub repository\n",
    "!git clone https://github.com/Simhadri123/HSIDwRD.git /kaggle/working/HSIDwRD\n",
    "!ls /kaggle/working/HSIDwRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install pytorch-gradual-warmup-lr from GitHub (not on PyPI)\n",
    "!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Install other requirements\n",
    "!pip install -r /kaggle/working/HSIDwRD/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Simple training script (REPLACED by enhanced version below)\n",
    "# This cell has been replaced by the enhanced training pipeline in cells 6-11\n",
    "# The new version includes:\n",
    "# - Real-time loss tracking and visualization\n",
    "# - Comprehensive training progress monitoring\n",
    "# - Better error handling and GPU utilization\n",
    "\n",
    "# Uncomment the line below ONLY if you want to run the basic training without visualization\n",
    "# !python /kaggle/working/HSIDwRD/train_denoising.py\n",
    "\n",
    "print(\"INFO: This basic training has been replaced by the enhanced version in cells 6-11\")\n",
    "print(\"Run cells 6-11 for training with loss visualization\")\n",
    "print(\"Run cells 12-15 for comprehensive test set evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Install additional plotting libraries\n",
    "!pip install matplotlib seaborn plotly yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Modified training script with loss tracking and visualization\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/HSIDwRD')\n",
    "\n",
    "# Import required libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Set up the environment\n",
    "from config import Config \n",
    "opt = Config('/kaggle/working/HSIDwRD/training.yml')\n",
    "\n",
    "gpus = ','.join([str(i) for i in opt.GPU])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import utils\n",
    "from dataloaders.data import get_training_data, get_validation_data\n",
    "from models import *\n",
    "from losses import CharbonnierLoss\n",
    "from tqdm import tqdm \n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Using GPU(s): {gpus}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Setup model and training parameters\n",
    "start_epoch = 1\n",
    "mode = opt.MODEL.MODE\n",
    "session = opt.MODEL.SESSION\n",
    "\n",
    "result_dir = os.path.join(opt.TRAINING.SAVE_DIR, mode, 'results', session)\n",
    "model_dir  = os.path.join(opt.TRAINING.SAVE_DIR, mode, 'models',  session)\n",
    "\n",
    "utils.mkdir(result_dir)\n",
    "utils.mkdir(model_dir)\n",
    "\n",
    "train_dir = opt.TRAINING.TRAIN_DIR\n",
    "val_dir   = opt.TRAINING.VAL_DIR\n",
    "save_images = opt.TRAINING.SAVE_IMAGES\n",
    "\n",
    "# Initialize loss tracking lists\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_psnrs = []\n",
    "epochs_logged = []\n",
    "iterations_logged = []\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Model directory: {model_dir}\")\n",
    "print(f\"Results directory: {result_dir}\")\n",
    "print(f\"Training data: {train_dir}\")\n",
    "print(f\"Validation data: {val_dir}\")\n",
    "\n",
    "######### Model ###########\n",
    "model_restoration = U_Net_3D()\n",
    "model_restoration.cuda()\n",
    "\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "\n",
    "new_lr = opt.OPTIM.LR_INITIAL\n",
    "optimizer = optim.AdamW(model_restoration.parameters(), lr=new_lr, betas=(0.9, 0.999),eps=1e-8, weight_decay=1e-4)\n",
    "\n",
    "print(f\"Model initialized with learning rate: {new_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ff161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Setup data loaders and training components\n",
    "######### Resume ###########\n",
    "if opt.TRAINING.RESUME:\n",
    "    path_chk_rest = utils.get_last_path(model_dir, '_latest.pth')\n",
    "    utils.load_checkpoint(model_restoration, path_chk_rest)\n",
    "    start_epoch = utils.load_start_epoch(path_chk_rest) + 1\n",
    "    lr = utils.load_optim(optimizer, path_chk_rest)\n",
    "    \n",
    "    for p in optimizer.param_groups: \n",
    "        p['lr'] = lr\n",
    "    warmup = False\n",
    "    new_lr = lr\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(f\"==> Resuming Training with learning rate: {new_lr}\")\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.OPTIM.NUM_EPOCHS-start_epoch+1, eta_min=1e-6)\n",
    "else:\n",
    "    warmup = True\n",
    "\n",
    "######### Scheduler ###########\n",
    "if warmup:\n",
    "    warmup_epochs = 3\n",
    "    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.OPTIM.NUM_EPOCHS-warmup_epochs, eta_min=1e-6)\n",
    "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
    "    scheduler.step()\n",
    "\n",
    "if len(device_ids) > 1:\n",
    "    model_restoration = nn.DataParallel(model_restoration, device_ids=device_ids)\n",
    "\n",
    "######### Loss ###########\n",
    "criterion = CharbonnierLoss().cuda()\n",
    "\n",
    "######### DataLoaders ###########\n",
    "img_options_train = {'patch_size': opt.TRAINING.TRAIN_PS}\n",
    "\n",
    "train_dataset = get_training_data(train_dir, opt.MODEL.RATIO, img_options_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.OPTIM.BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "val_dataset = get_validation_data(val_dir, opt.MODEL.RATIO)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "print('Data loaders ready!')\n",
    "print(f'Training batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')\n",
    "print(f'Training from epoch {start_epoch} to {opt.OPTIM.NUM_EPOCHS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f83733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Training loop with loss tracking\n",
    "def plot_training_progress(train_losses, val_losses, val_psnrs, epochs_logged, save_path=None):\n",
    "    \"\"\"Plot training progress with dual y-axis for losses and PSNR\"\"\"\n",
    "    \n",
    "    # Create matplotlib figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training loss\n",
    "    if train_losses:\n",
    "        ax1.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss Over Time')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Plot validation loss and PSNR\n",
    "    if val_losses and val_psnrs:\n",
    "        ax2_twin = ax2.twinx()\n",
    "        \n",
    "        line1 = ax2.plot(epochs_logged, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        line2 = ax2_twin.plot(epochs_logged, val_psnrs, 'g-', label='Validation PSNR', linewidth=2)\n",
    "        \n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Validation Loss', color='r')\n",
    "        ax2_twin.set_ylabel('PSNR (dB)', color='g')\n",
    "        ax2.set_title('Validation Metrics Over Time')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Combine legends\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax2.legend(lines, labels, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_interactive_progress(train_losses, val_losses, val_psnrs, epochs_logged):\n",
    "    \"\"\"Create interactive plotly plots for training progress\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Training Loss', 'Validation Loss', 'Validation PSNR', 'Combined View'),\n",
    "        specs=[[{}, {}], [{}, {\"secondary_y\": True}]]\n",
    "    )\n",
    "    \n",
    "    # Training loss\n",
    "    if train_losses:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(1, len(train_losses) + 1)), y=train_losses, \n",
    "                      mode='lines', name='Training Loss', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Validation loss\n",
    "    if val_losses:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs_logged, y=val_losses, \n",
    "                      mode='lines+markers', name='Validation Loss', line=dict(color='red')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Validation PSNR\n",
    "    if val_psnrs:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs_logged, y=val_psnrs, \n",
    "                      mode='lines+markers', name='Validation PSNR', line=dict(color='green')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Combined view with dual y-axis\n",
    "    if val_losses and val_psnrs:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs_logged, y=val_losses, \n",
    "                      mode='lines+markers', name='Val Loss', line=dict(color='red')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs_logged, y=val_psnrs, \n",
    "                      mode='lines+markers', name='Val PSNR', line=dict(color='green'),\n",
    "                      yaxis='y2'),\n",
    "            row=2, col=2, secondary_y=True\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Training Progress Dashboard\")\n",
    "    fig.update_xaxes(title_text=\"Epoch\")\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"PSNR (dB)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"PSNR (dB)\", row=2, col=2, secondary_y=True)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "print(\"Plotting functions defined!\")\n",
    "\n",
    "# Initialize variables for training\n",
    "mixup = utils.MixUp_AUG()\n",
    "best_psnr = 0\n",
    "best_epoch = 0\n",
    "best_iter = 0\n",
    "\n",
    "eval_now = len(train_loader) - 1\n",
    "print(f\"Evaluation after every {eval_now} iterations\")\n",
    "print(f\"Starting training from epoch {start_epoch} to {opt.OPTIM.NUM_EPOCHS}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622be968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Main training loop\n",
    "for epoch in range(start_epoch, opt.OPTIM.NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    train_id = 1\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{opt.OPTIM.NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "        \n",
    "    for i, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}\")): \n",
    "        # Zero gradients\n",
    "        for param in model_restoration.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        target = torch.clamp(data[0].cuda(), 0, 1)\n",
    "        input_ = torch.clamp(data[1].cuda(), 0, 1)\n",
    "\n",
    "        if epoch > 5:\n",
    "            target, input_ = mixup.aug(target, input_)\n",
    "\n",
    "        restored = model_restoration(input_.unsqueeze(1))[:,0]\n",
    "        restored = torch.clamp(restored, 0, 1)  \n",
    "        \n",
    "        loss = criterion(restored, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        #### Evaluation ####\n",
    "        if epoch % 10 == 0 and i % eval_now == 0 and i > 0:\n",
    "            if save_images:\n",
    "                utils.mkdir(result_dir + f'/{epoch}/{i}')\n",
    "                \n",
    "            model_restoration.eval()\n",
    "            with torch.no_grad():\n",
    "                psnr_val = []\n",
    "                val_loss_total = 0\n",
    "                \n",
    "                for ii, data_val in enumerate(val_loader):\n",
    "                    target = data_val[0].cuda()\n",
    "                    input_ = data_val[1].cuda()\n",
    "                    filenames = data_val[2]\n",
    "\n",
    "                    restored = model_restoration(input_.unsqueeze(1))[:,0]\n",
    "                    restored = torch.clamp(restored, 0, 1)\n",
    "                    \n",
    "                    # Calculate validation loss\n",
    "                    val_loss = criterion(restored, target)\n",
    "                    val_loss_total += val_loss.item()\n",
    "                    \n",
    "                    psnr_val.append(utils.batch_PSNR(restored, target, 1.))\n",
    "\n",
    "                    if save_images:\n",
    "                        target_np = target.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "                        input_np = input_.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "                        restored_np = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "                        \n",
    "                        for batch in range(input_np.shape[0]):\n",
    "                            temp = np.concatenate((input_np[batch]*255, restored_np[batch]*255, target_np[batch]*255), axis=1)\n",
    "                            utils.save_img(os.path.join(result_dir, str(epoch), str(i), filenames[batch][:-4] + '.jpg'), temp.astype(np.uint8))\n",
    "\n",
    "                psnr_val = sum(psnr_val) / len(psnr_val)\n",
    "                val_loss_avg = val_loss_total / len(val_loader)\n",
    "                \n",
    "                # Store validation metrics\n",
    "                val_losses.append(val_loss_avg)\n",
    "                val_psnrs.append(psnr_val)\n",
    "                epochs_logged.append(epoch)\n",
    "                \n",
    "                if psnr_val > best_psnr:\n",
    "                    best_psnr = psnr_val\n",
    "                    best_epoch = epoch\n",
    "                    best_iter = i \n",
    "                    torch.save({\n",
    "                        'epoch': epoch, \n",
    "                        'state_dict': model_restoration.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict()\n",
    "                    }, os.path.join(model_dir, \"model_best.pth\"))\n",
    "\n",
    "                print(f\"[Ep {epoch} it {i}\\t PSNR: {psnr_val:.4f}\\t Val Loss: {val_loss_avg:.4f}] ----  [Best Ep {best_epoch} Best PSNR {best_psnr:.4f}]\")\n",
    "            \n",
    "            model_restoration.train()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store training loss for this epoch\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    current_lr = scheduler.get_lr()[0]\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Epoch: {epoch}\\tTime: {epoch_time:.2f}s\\tLoss: {avg_epoch_loss:.6f}\\tLR: {current_lr:.6f}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Plot progress every 10 epochs or at the end\n",
    "    if epoch % 10 == 0 or epoch == opt.OPTIM.NUM_EPOCHS:\n",
    "        print(f\"Plotting training progress at epoch {epoch}...\")\n",
    "        plot_training_progress(train_losses, val_losses, val_psnrs, epochs_logged, \n",
    "                             save_path=os.path.join(result_dir, f'training_progress_epoch_{epoch}.png'))\n",
    "\n",
    "    # Save checkpoint periodically\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch, \n",
    "            'state_dict': model_restoration.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_psnrs': val_psnrs,\n",
    "            'epochs_logged': epochs_logged\n",
    "        }, os.path.join(model_dir, \"model_latest.pth\"))   \n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch, \n",
    "            'state_dict': model_restoration.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_psnrs': val_psnrs,\n",
    "            'epochs_logged': epochs_logged\n",
    "        }, os.path.join(model_dir, f\"model_epoch_{epoch}.pth\"))\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best PSNR: {best_psnr:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# Save final training history\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_psnrs': val_psnrs,\n",
    "    'epochs_logged': epochs_logged,\n",
    "    'best_psnr': best_psnr,\n",
    "    'best_epoch': best_epoch\n",
    "}\n",
    "\n",
    "with open(os.path.join(result_dir, 'training_history.json'), 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "\n",
    "print(f\"Training history saved to: {os.path.join(result_dir, 'training_history.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Final training visualization\n",
    "print(\"Creating final training visualization...\")\n",
    "plot_training_progress(train_losses, val_losses, val_psnrs, epochs_logged, \n",
    "                     save_path=os.path.join(result_dir, 'final_training_progress.png'))\n",
    "\n",
    "print(\"Creating interactive dashboard...\")\n",
    "plot_interactive_progress(train_losses, val_losses, val_psnrs, epochs_logged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6623e",
   "metadata": {},
   "source": [
    "# Test Set Inference and Evaluation\n",
    "\n",
    "Now that training is complete, let's run inference on the test set to evaluate our model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fdf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Load best model for testing\n",
    "def load_model_for_testing(model_path, device='cuda'):\n",
    "    \"\"\"Load the best trained model for testing\"\"\"\n",
    "    model = U_Net_3D()\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        checkpoint = torch.load(model_path)\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            epoch = checkpoint.get('epoch', 'unknown')\n",
    "            print(f\"Model loaded from epoch {epoch}\")\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(\"Model loaded successfully\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for inference\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load the best model\n",
    "best_model_path = os.path.join(model_dir, \"model_best.pth\")\n",
    "test_model = load_model_for_testing(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573563bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Setup test dataset and evaluation metrics\n",
    "import scipy.io as sio\n",
    "from sklearn.metrics import mean_squared_error, peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "\n",
    "def calculate_metrics(gt, restored):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Ensure data is in correct format and range\n",
    "    gt = np.clip(gt, 0, 1)\n",
    "    restored = np.clip(restored, 0, 1)\n",
    "    \n",
    "    # PSNR (Peak Signal-to-Noise Ratio)\n",
    "    mse = mean_squared_error(gt.flatten(), restored.flatten())\n",
    "    if mse == 0:\n",
    "        psnr = float('inf')\n",
    "    else:\n",
    "        psnr = 20 * np.log10(1.0 / np.sqrt(mse))\n",
    "    metrics['PSNR'] = psnr\n",
    "    \n",
    "    # MSE (Mean Squared Error)\n",
    "    metrics['MSE'] = mse\n",
    "    \n",
    "    # SSIM (Structural Similarity Index) - calculate per channel and average\n",
    "    if len(gt.shape) == 3:  # Multi-channel\n",
    "        ssim_values = []\n",
    "        for i in range(gt.shape[0]):  # Assuming channels first\n",
    "            ssim_val = ssim(gt[i], restored[i], data_range=1.0)\n",
    "            ssim_values.append(ssim_val)\n",
    "        metrics['SSIM'] = np.mean(ssim_values)\n",
    "    else:  # Single channel\n",
    "        metrics['SSIM'] = ssim(gt, restored, data_range=1.0)\n",
    "    \n",
    "    # SAM (Spectral Angle Mapper) for hyperspectral images\n",
    "    if len(gt.shape) == 3:\n",
    "        gt_flat = gt.reshape(gt.shape[0], -1)  # (channels, pixels)\n",
    "        restored_flat = restored.reshape(restored.shape[0], -1)\n",
    "        \n",
    "        # Calculate spectral angle for each pixel\n",
    "        dot_product = np.sum(gt_flat * restored_flat, axis=0)\n",
    "        norm_gt = np.linalg.norm(gt_flat, axis=0)\n",
    "        norm_restored = np.linalg.norm(restored_flat, axis=0)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        valid_pixels = (norm_gt > 0) & (norm_restored > 0)\n",
    "        cos_angles = np.zeros_like(dot_product)\n",
    "        cos_angles[valid_pixels] = dot_product[valid_pixels] / (norm_gt[valid_pixels] * norm_restored[valid_pixels])\n",
    "        \n",
    "        # Clip to [-1, 1] to avoid numerical errors in arccos\n",
    "        cos_angles = np.clip(cos_angles, -1, 1)\n",
    "        angles = np.arccos(cos_angles)\n",
    "        metrics['SAM'] = np.mean(angles[valid_pixels]) if np.any(valid_pixels) else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_test_results(results, save_dir):\n",
    "    \"\"\"Save test results to files\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save metrics summary\n",
    "    metrics_file = os.path.join(save_dir, 'test_metrics.json')\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Create metrics summary table\n",
    "    if 'per_image_metrics' in results:\n",
    "        metrics_df = []\n",
    "        for filename, metrics in results['per_image_metrics'].items():\n",
    "            row = {'filename': filename}\n",
    "            row.update(metrics)\n",
    "            metrics_df.append(row)\n",
    "        \n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(metrics_df)\n",
    "        csv_file = os.path.join(save_dir, 'detailed_metrics.csv')\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"Detailed metrics saved to: {csv_file}\")\n",
    "    \n",
    "    print(f\"Test results saved to: {save_dir}\")\n",
    "\n",
    "# Setup test data directory (you may need to adjust this path)\n",
    "test_dir = val_dir  # Using validation data as test data for demonstration\n",
    "test_result_dir = os.path.join(result_dir, 'test_results')\n",
    "utils.mkdir(test_result_dir)\n",
    "\n",
    "print(f\"Test data directory: {test_dir}\")\n",
    "print(f\"Test results will be saved to: {test_result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Run inference on test set\n",
    "def run_test_inference(model, test_loader, save_images=True, save_dir=None):\n",
    "    \"\"\"Run inference on test dataset and calculate metrics\"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"No model available for testing\")\n",
    "        return None\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_metrics = []\n",
    "    per_image_metrics = {}\n",
    "    total_files = len(test_loader)\n",
    "    \n",
    "    print(f\"Running inference on {total_files} test samples...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ii, data_test in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            hsi_gt = data_test[0].cuda()\n",
    "            hsi_noisy = data_test[1].cuda()\n",
    "            filenames = data_test[2]\n",
    "            \n",
    "            # Run inference\n",
    "            hsi_restored = model(hsi_noisy.unsqueeze(1))[:,0]\n",
    "            hsi_restored = torch.clamp(hsi_restored, 0, 1)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            hsi_gt_np = hsi_gt.cpu().detach().numpy()\n",
    "            hsi_noisy_np = hsi_noisy.cpu().detach().numpy()\n",
    "            hsi_restored_np = hsi_restored.cpu().detach().numpy()\n",
    "            \n",
    "            # Process each item in the batch\n",
    "            for batch_idx in range(len(hsi_gt_np)):\n",
    "                gt = hsi_gt_np[batch_idx]\n",
    "                noisy = hsi_noisy_np[batch_idx]\n",
    "                restored = hsi_restored_np[batch_idx]\n",
    "                filename = filenames[batch_idx]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = calculate_metrics(gt, restored)\n",
    "                metrics_noisy = calculate_metrics(gt, noisy)\n",
    "                \n",
    "                # Store metrics\n",
    "                per_image_metrics[filename] = {\n",
    "                    'PSNR_restored': metrics['PSNR'],\n",
    "                    'PSNR_noisy': metrics_noisy['PSNR'],\n",
    "                    'PSNR_improvement': metrics['PSNR'] - metrics_noisy['PSNR'],\n",
    "                    'MSE_restored': metrics['MSE'],\n",
    "                    'MSE_noisy': metrics_noisy['MSE'],\n",
    "                    'SSIM_restored': metrics['SSIM'],\n",
    "                    'SSIM_noisy': metrics_noisy['SSIM'],\n",
    "                    'SSIM_improvement': metrics['SSIM'] - metrics_noisy['SSIM']\n",
    "                }\n",
    "                \n",
    "                if 'SAM' in metrics:\n",
    "                    per_image_metrics[filename]['SAM_restored'] = metrics['SAM']\n",
    "                    per_image_metrics[filename]['SAM_noisy'] = metrics_noisy['SAM']\n",
    "                    per_image_metrics[filename]['SAM_improvement'] = metrics_noisy['SAM'] - metrics['SAM']\n",
    "                \n",
    "                all_metrics.append(metrics)\n",
    "                \n",
    "                # Save images if requested\n",
    "                if save_images and save_dir:\n",
    "                    # Save individual channels or RGB representation\n",
    "                    if len(gt.shape) == 3 and gt.shape[0] > 3:  # Hyperspectral\n",
    "                        # Save as .mat file for hyperspectral data\n",
    "                        denoised_hsi = np.rot90(restored[:, ::-1], axes=(-2,-1))\n",
    "                        mat_path = os.path.join(save_dir, filename[:-4] + '_restored.mat')\n",
    "                        sio.savemat(mat_path, {'R_hsi': np.transpose(denoised_hsi, (1,2,0))})\n",
    "                        \n",
    "                        # Also save RGB representation (using specific bands)\n",
    "                        if gt.shape[0] >= 50:  # Ensure we have enough bands\n",
    "                            # Select RGB-like bands (adjust indices based on your data)\n",
    "                            rgb_bands = [30, 20, 10]  # Example band selection\n",
    "                            rgb_gt = gt[rgb_bands].transpose(1, 2, 0)\n",
    "                            rgb_noisy = noisy[rgb_bands].transpose(1, 2, 0)\n",
    "                            rgb_restored = restored[rgb_bands].transpose(1, 2, 0)\n",
    "                            \n",
    "                            # Normalize for display\n",
    "                            rgb_gt = (rgb_gt * 255).astype(np.uint8)\n",
    "                            rgb_noisy = (rgb_noisy * 255).astype(np.uint8)\n",
    "                            rgb_restored = (rgb_restored * 255).astype(np.uint8)\n",
    "                            \n",
    "                            # Create comparison image\n",
    "                            comparison = np.concatenate([rgb_noisy, rgb_restored, rgb_gt], axis=1)\n",
    "                            cv2.imwrite(os.path.join(save_dir, filename[:-4] + '_comparison.jpg'), comparison)\n",
    "                    else:\n",
    "                        # Regular RGB or grayscale image\n",
    "                        if len(gt.shape) == 3:\n",
    "                            gt_img = (gt.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "                            noisy_img = (noisy.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "                            restored_img = (restored.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "                        else:\n",
    "                            gt_img = (gt * 255).astype(np.uint8)\n",
    "                            noisy_img = (noisy * 255).astype(np.uint8)\n",
    "                            restored_img = (restored * 255).astype(np.uint8)\n",
    "                        \n",
    "                        # Create comparison\n",
    "                        comparison = np.concatenate([noisy_img, restored_img, gt_img], axis=1)\n",
    "                        cv2.imwrite(os.path.join(save_dir, filename[:-4] + '_comparison.jpg'), comparison)\n",
    "                \n",
    "                # Print progress for first few samples\n",
    "                if ii < 5:\n",
    "                    print(f\"{filename}:\")\n",
    "                    print(f\"   PSNR: {metrics_noisy['PSNR']:.2f} -> {metrics['PSNR']:.2f} dB (improvement: {metrics['PSNR'] - metrics_noisy['PSNR']:.2f})\")\n",
    "                    print(f\"   SSIM: {metrics_noisy['SSIM']:.4f} -> {metrics['SSIM']:.4f} (improvement: {metrics['SSIM'] - metrics_noisy['SSIM']:.4f})\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {}\n",
    "    if all_metrics:\n",
    "        for key in all_metrics[0].keys():\n",
    "            avg_metrics[f'avg_{key}'] = np.mean([m[key] for m in all_metrics])\n",
    "            avg_metrics[f'std_{key}'] = np.std([m[key] for m in all_metrics])\n",
    "    \n",
    "    # Calculate improvement statistics\n",
    "    improvements = {\n",
    "        'avg_PSNR_improvement': np.mean([m['PSNR_improvement'] for m in per_image_metrics.values()]),\n",
    "        'avg_SSIM_improvement': np.mean([m['SSIM_improvement'] for m in per_image_metrics.values()]),\n",
    "        'std_PSNR_improvement': np.std([m['PSNR_improvement'] for m in per_image_metrics.values()]),\n",
    "        'std_SSIM_improvement': np.std([m['SSIM_improvement'] for m in per_image_metrics.values()])\n",
    "    }\n",
    "    \n",
    "    if any('SAM_improvement' in m for m in per_image_metrics.values()):\n",
    "        improvements['avg_SAM_improvement'] = np.mean([m['SAM_improvement'] for m in per_image_metrics.values() if 'SAM_improvement' in m])\n",
    "        improvements['std_SAM_improvement'] = np.std([m['SAM_improvement'] for m in per_image_metrics.values() if 'SAM_improvement' in m])\n",
    "    \n",
    "    results = {\n",
    "        'average_metrics': avg_metrics,\n",
    "        'improvements': improvements,\n",
    "        'per_image_metrics': per_image_metrics,\n",
    "        'total_samples': total_files\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = get_validation_data(test_dir, opt.MODEL.RATIO)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "print(f\"Test dataset ready with {len(test_loader)} samples\")\n",
    "\n",
    "# Run inference\n",
    "test_results = run_test_inference(test_model, test_loader, save_images=True, save_dir=test_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Display and analyze test results\n",
    "def display_test_results(results):\n",
    "    \"\"\"Display comprehensive test results with visualizations\"\"\"\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"No test results available\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_metrics = results['average_metrics']\n",
    "    improvements = results['improvements']\n",
    "    \n",
    "    print(f\"\\nAVERAGE PERFORMANCE METRICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"PSNR: {avg_metrics.get('avg_PSNR', 0):.4f} ± {avg_metrics.get('std_PSNR', 0):.4f} dB\")\n",
    "    print(f\"MSE:  {avg_metrics.get('avg_MSE', 0):.6f} ± {avg_metrics.get('std_MSE', 0):.6f}\")\n",
    "    print(f\"SSIM: {avg_metrics.get('avg_SSIM', 0):.4f} ± {avg_metrics.get('std_SSIM', 0):.4f}\")\n",
    "    if 'avg_SAM' in avg_metrics:\n",
    "        print(f\"SAM:  {avg_metrics.get('avg_SAM', 0):.4f} ± {avg_metrics.get('std_SAM', 0):.4f} rad\")\n",
    "    \n",
    "    print(f\"\\nIMPROVEMENT OVER NOISY INPUT:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"PSNR Improvement: {improvements.get('avg_PSNR_improvement', 0):.4f} ± {improvements.get('std_PSNR_improvement', 0):.4f} dB\")\n",
    "    print(f\"SSIM Improvement: {improvements.get('avg_SSIM_improvement', 0):.4f} ± {improvements.get('std_SSIM_improvement', 0):.4f}\")\n",
    "    if 'avg_SAM_improvement' in improvements:\n",
    "        print(f\"SAM Improvement:  {improvements.get('avg_SAM_improvement', 0):.4f} ± {improvements.get('std_SAM_improvement', 0):.4f} rad\")\n",
    "    \n",
    "    print(f\"\\nTOTAL SAMPLES PROCESSED: {results['total_samples']}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    per_image = results['per_image_metrics']\n",
    "    \n",
    "    if per_image:\n",
    "        # Extract data for plotting\n",
    "        filenames = list(per_image.keys())\n",
    "        psnr_restored = [per_image[f]['PSNR_restored'] for f in filenames]\n",
    "        psnr_noisy = [per_image[f]['PSNR_noisy'] for f in filenames]\n",
    "        psnr_improvement = [per_image[f]['PSNR_improvement'] for f in filenames]\n",
    "        ssim_restored = [per_image[f]['SSIM_restored'] for f in filenames]\n",
    "        ssim_noisy = [per_image[f]['SSIM_noisy'] for f in filenames]\n",
    "        ssim_improvement = [per_image[f]['SSIM_improvement'] for f in filenames]\n",
    "        \n",
    "        # Create matplotlib visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # PSNR comparison\n",
    "        axes[0, 0].bar(range(len(filenames)), psnr_noisy, alpha=0.7, label='Noisy', color='red')\n",
    "        axes[0, 0].bar(range(len(filenames)), psnr_restored, alpha=0.7, label='Restored', color='blue')\n",
    "        axes[0, 0].set_title('PSNR Comparison')\n",
    "        axes[0, 0].set_ylabel('PSNR (dB)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # PSNR improvement\n",
    "        axes[0, 1].bar(range(len(filenames)), psnr_improvement, color='green', alpha=0.7)\n",
    "        axes[0, 1].set_title('PSNR Improvement')\n",
    "        axes[0, 1].set_ylabel('PSNR Improvement (dB)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # PSNR distribution\n",
    "        axes[0, 2].hist([psnr_noisy, psnr_restored], bins=15, alpha=0.7, \n",
    "                       label=['Noisy', 'Restored'], color=['red', 'blue'])\n",
    "        axes[0, 2].set_title('PSNR Distribution')\n",
    "        axes[0, 2].set_xlabel('PSNR (dB)')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].legend()\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # SSIM comparison\n",
    "        axes[1, 0].bar(range(len(filenames)), ssim_noisy, alpha=0.7, label='Noisy', color='red')\n",
    "        axes[1, 0].bar(range(len(filenames)), ssim_restored, alpha=0.7, label='Restored', color='blue')\n",
    "        axes[1, 0].set_title('SSIM Comparison')\n",
    "        axes[1, 0].set_ylabel('SSIM')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # SSIM improvement\n",
    "        axes[1, 1].bar(range(len(filenames)), ssim_improvement, color='green', alpha=0.7)\n",
    "        axes[1, 1].set_title('SSIM Improvement')\n",
    "        axes[1, 1].set_ylabel('SSIM Improvement')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # SSIM distribution\n",
    "        axes[1, 2].hist([ssim_noisy, ssim_restored], bins=15, alpha=0.7,\n",
    "                       label=['Noisy', 'Restored'], color=['red', 'blue'])\n",
    "        axes[1, 2].set_title('SSIM Distribution')\n",
    "        axes[1, 2].set_xlabel('SSIM')\n",
    "        axes[1, 2].set_ylabel('Frequency')\n",
    "        axes[1, 2].legend()\n",
    "        axes[1, 2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(test_result_dir, 'test_results_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Interactive Plotly visualization\n",
    "        fig_plotly = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('PSNR Metrics', 'SSIM Metrics', 'Improvements', 'Correlation Analysis'),\n",
    "            specs=[[{}, {}], [{}, {}]]\n",
    "        )\n",
    "        \n",
    "        # PSNR metrics\n",
    "        fig_plotly.add_trace(\n",
    "            go.Scatter(x=list(range(len(filenames))), y=psnr_noisy, \n",
    "                      mode='lines+markers', name='PSNR Noisy', line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig_plotly.add_trace(\n",
    "            go.Scatter(x=list(range(len(filenames))), y=psnr_restored, \n",
    "                      mode='lines+markers', name='PSNR Restored', line=dict(color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # SSIM metrics\n",
    "        fig_plotly.add_trace(\n",
    "            go.Scatter(x=list(range(len(filenames))), y=ssim_noisy, \n",
    "                      mode='lines+markers', name='SSIM Noisy', line=dict(color='red')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        fig_plotly.add_trace(\n",
    "            go.Scatter(x=list(range(len(filenames))), y=ssim_restored, \n",
    "                      mode='lines+markers', name='SSIM Restored', line=dict(color='blue')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Improvements\n",
    "        fig_plotly.add_trace(\n",
    "            go.Bar(x=list(range(len(filenames))), y=psnr_improvement, \n",
    "                   name='PSNR Improvement', marker_color='green'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Correlation between PSNR and SSIM improvements\n",
    "        fig_plotly.add_trace(\n",
    "            go.Scatter(x=psnr_improvement, y=ssim_improvement, \n",
    "                      mode='markers', name='PSNR vs SSIM Improvement',\n",
    "                      marker=dict(size=8, color='purple')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig_plotly.update_layout(height=800, showlegend=True, title_text=\"Test Results Analysis Dashboard\")\n",
    "        fig_plotly.show()\n",
    "\n",
    "# Display results\n",
    "if test_results:\n",
    "    display_test_results(test_results)\n",
    "    \n",
    "    # Save results\n",
    "    save_test_results(test_results, test_result_dir)\n",
    "    \n",
    "    print(f\"\\nAll test results saved to: {test_result_dir}\")\n",
    "    print(f\"Check the following files:\")\n",
    "    print(f\"   - test_metrics.json: Overall metrics summary\")\n",
    "    print(f\"   - detailed_metrics.csv: Per-image detailed metrics\")\n",
    "    print(f\"   - test_results_analysis.png: Static analysis plots\")\n",
    "    print(f\"   - Individual comparison images for visual inspection\")\n",
    "else:\n",
    "    print(\"No test results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e90de0",
   "metadata": {},
   "source": [
    "# Summary and Conclusion\n",
    "\n",
    "## Complete Pipeline Overview\n",
    "\n",
    "This notebook provides a comprehensive pipeline for HSI denoising:\n",
    "\n",
    "### 1. **Training with Visualization**\n",
    "- Real-time loss tracking for both training and validation\n",
    "- Interactive plots showing training progress\n",
    "- Automatic model checkpointing and best model saving\n",
    "\n",
    "### 2. **Test Set Evaluation**\n",
    "- Comprehensive metrics: PSNR, SSIM, MSE, and SAM\n",
    "- Before/after comparisons with improvement statistics\n",
    "- Visual analysis with both static and interactive plots\n",
    "- Detailed per-image and aggregate results\n",
    "\n",
    "### 3. **Key Features**\n",
    "- **GPU acceleration** with multi-GPU support\n",
    "- **Comprehensive metrics** for hyperspectral image evaluation\n",
    "- **Visual outputs** including comparison images and analysis plots\n",
    "- **Robust error handling** and progress tracking\n",
    "- **Flexible configuration** via YAML config files\n",
    "\n",
    "### 4. **Output Files**\n",
    "- `training_history.json`: Complete training metrics\n",
    "- `test_metrics.json`: Comprehensive test results\n",
    "- `detailed_metrics.csv`: Per-image breakdown\n",
    "- Visualization plots and comparison images"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
